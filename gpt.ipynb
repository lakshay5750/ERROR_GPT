{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6fc4f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in e:\\ananconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in e:\\ananconda3\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in e:\\ananconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (3.11.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in e:\\ananconda3\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in e:\\ananconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in e:\\ananconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in e:\\ananconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in e:\\ananconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\ananconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\ananconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\ananconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\ananconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in e:\\ananconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in e:\\ananconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in e:\\ananconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in e:\\ananconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\ananconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\ananconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\ananconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded6e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a31448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt','r',encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f27ce92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dr. A.P.J. Abdul Kalam, born Avul Pakir Jainulabdeen Abdul Kalam, embarked on a remarkable journey from humble beginnings to become one of India\\'s most revered figures. His life story is a testament to perseverance, dedication, and an unwavering commitment to national development.\\n\\nHere\\'s a detailed look at his inspiring journey:\\n\\n### Early Life and Education (1931-1960)\\n\\n* **Birth and Childhood:** Avul Pakir Jainulabdeen Abdul Kalam was born on October 15, 1931, in Rameswaram, Tamil Nadu, into a Tamil Muslim family. His father, Jainulabdeen, was a boat owner and imam, and his mother, Ashiamma, was a housewife. Despite his ancestors being wealthy traders, the family faced financial hardship during his childhood. To support his family, young Kalam sold newspapers.\\n* **Early Education:** He attended Schwartz Higher Secondary School in Ramanathapuram, where he was described as a bright and hardworking student with a strong desire to learn, especially mathematics.\\n* **Higher Education:**\\n    * He graduated in Physics from Saint Joseph\\'s College, Tiruchirappalli, in 1954.\\n    * Driven by his passion, he moved to Madras (now Chennai) in 1955 to study aerospace engineering at the Madras Institute of Technology (MIT).\\n    * During a senior-year project, he faced a challenge from his dean but completed the project successfully within a tight deadline, impressing his professor.\\n* **A Dream Undone:** Kalam\\'s dream was to become a fighter pilot in the Indian Air Force, but he narrowly missed the cut, placing ninth in the qualifiers when only eight positions were available.\\n\\n### Scientific Career (1958-2002)\\n\\nAbdul Kalam\\'s scientific career spanned over four decades, primarily at the Defence Research and Development Organisation (DRDO) and the Indian Space Research Organisation (ISRO), where he played pivotal roles in India\\'s space and missile programs.\\n\\n* **DRDO (1958-1969):**\\n    * After graduating from MIT in 1960, he joined the Aeronautical Development Establishment of DRDO as a scientist.\\n    * His early work involved designing a small hovercraft. However, he felt his role wasn\\'t fully satisfying.\\n    * He also initiated an independent project on an expandable rocket in 1965.\\n* **ISRO (1969-1982):**\\n    * In 1969, Kalam transferred to ISRO, a move that proved to be a turning point.\\n    * He became the **Project Director of India\\'s first Satellite Launch Vehicle (SLV-III)**. Under his leadership, the SLV-III successfully placed the **Rohini satellite** into orbit in July 1980, marking a significant milestone in India\\'s space journey.\\n    * He also contributed to the development of the Polar Satellite Launch Vehicle (PSLV).\\n* **Return to DRDO and the \"Missile Man of India\" (1982-1999):**\\n    * Kalam returned to DRDO in 1982 as the Director of the Defence Research and Development Laboratory (DRDL).\\n    * He was instrumental in conceiving and leading the **Integrated Guided Missile Development Programme (IGMDP)**. This ambitious program aimed at achieving self-reliance in missile technology.\\n    * Under his guidance, India successfully developed a series of indigenous missiles, including:\\n        * **Prithvi:** A short-range surface-to-surface ballistic missile.\\n        * **Agni:** An intermediate-range ballistic missile.\\n        * **Trishul:** A short-range surface-to-air missile.\\n        * **Akash:** A medium-range surface-to-air missile.\\n        * **Nag:** An anti-tank guided missile.\\n    * His leadership in these projects earned him the title **\"Missile Man of India.\"**\\n    * From 1992 to 1999, he served as the **Chief Scientific Adviser to the Prime Minister and Secretary of the Department of Defence Research & Development**.\\n    * **Pokhran-II Nuclear Tests (1998):** Kalam played a pivotal organizational, technical, and political role in the Pokhran-II nuclear tests in May 1998. These tests made India a full-fledged nuclear weapon state and significantly boosted Kalam\\'s national recognition.\\n\\n### Presidency (2002-2007)\\n\\n* **The People\\'s President:** In 2002, A.P.J. Abdul Kalam was nominated as the presidential candidate by the ruling National Democratic Alliance (NDA) and received support from the Indian National Congress. He was overwhelmingly elected as the **11th President of India**, serving from July 25, 2002, to July 25, 2007.\\n* He was the **first scientist and the first bachelor** to become India\\'s President.\\n* During his tenure, he was affectionately known as the **\"People\\'s President\"** due to his accessible nature, his focus on inspiring the youth, and his numerous interactions with common citizens, particularly students.\\n* He actively promoted his vision for India, encapsulated in his book **\"India 2020: A Vision for the New Millennium,\"** which outlined a roadmap for transforming India into a developed nation.\\n* He handled a controversial decision by recommending President\\'s rule in Bihar in 2005.\\n\\n### Post-Presidency (2007-2015)\\n\\nEven after his presidency, Dr. Kalam remained highly active and dedicated to public service. He declined the opportunity for a second term despite widespread public support.\\n\\n* **Education and Mentorship:** He returned to his passion for teaching, serving as a visiting professor at various prestigious institutions, including:\\n    * Indian Institute of Management (IIM) Ahmedabad\\n    * IIM Shillong\\n    * IIM Indore\\n    * Indian Institute of Science (IISc) Bangalore\\n    * Anna University\\n* **Writing and Public Speaking:** He continued to write influential books and deliver numerous motivational lectures across India and abroad, particularly to students. His focus remained on inspiring youth to pursue their dreams and contribute to nation-building.\\n* **Key Initiatives:** He continued advocating for concepts like \"Provision of Urban Amenities to Rural Areas (PURA)\" and \"Technology Vision 2020.\"\\n\\n### Demise (2015)\\n\\n* **Last Moments:** On July 27, 2015, while delivering a lecture on \"Creating a Livable Planet Earth\" to students at the Indian Institute of Management Shillong, Dr. Kalam collapsed from a massive cardiac arrest.\\n* **National Mourning:** His sudden demise led to widespread grief across India and the world. He was laid to rest with full state honours in his hometown of Rameswaram, with thousands attending his funeral.\\n\\n### Legacy and Awards\\n\\nDr. Kalam\\'s legacy is immense and multifaceted:\\n\\n* **Missile Man of India:** For his monumental contributions to India\\'s ballistic missile and space rocketry programs.\\n* **People\\'s President:** For his approachable demeanor, dedication to the common person, and inspiring interactions, especially with students.\\n* **Visionary Leader:** His \"India 2020\" vision continues to be a guiding light for national development.\\n* **Author:** His books, most notably **\"Wings of Fire\" (autobiography)**, \"Ignited Minds,\" and \"India 2020,\" have inspired millions.\\n* **Awards and Honours:** He received numerous prestigious awards, including:\\n    * **Padma Bhushan (1981)**\\n    * **Padma Vibhushan (1990)**\\n    * **Bharat Ratna (1997)** – India\\'s highest civilian honor.\\n    * Indira Gandhi Award for National Integration (1997)\\n    * Veer Savarkar Award (1998)\\n    * King Charles II Medal (Royal Society, UK, 2007)\\n    * Doctor of Science (Edinburgh University, UK, 2014)\\n\\nDr. A.P.J. Abdul Kalam\\'s journey is a powerful narrative of how talent, hard work, and an unyielding spirit can overcome adversity and contribute profoundly to a nation\\'s progress. He remains an eternal source of inspiration for generations, embodying the spirit of scientific inquiry, national pride, and selfless service.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff3e17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a897ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51760e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.legacy.preprocessing.text.Tokenizer at 0x1d8f75be810>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d27f985e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'a': 2,\n",
       " 'the': 3,\n",
       " 'to': 4,\n",
       " 'and': 5,\n",
       " 'his': 6,\n",
       " 'he': 7,\n",
       " 'in': 8,\n",
       " 'of': 9,\n",
       " 'india': 10,\n",
       " 'missile': 11,\n",
       " 'was': 12,\n",
       " 'for': 13,\n",
       " 'kalam': 14,\n",
       " 'from': 15,\n",
       " 'national': 16,\n",
       " 'development': 17,\n",
       " 'as': 18,\n",
       " 'on': 19,\n",
       " \"india's\": 20,\n",
       " 'an': 21,\n",
       " 'abdul': 22,\n",
       " 'at': 23,\n",
       " 'indian': 24,\n",
       " 'dr': 25,\n",
       " 'with': 26,\n",
       " \"kalam's\": 27,\n",
       " 'drdo': 28,\n",
       " 'president': 29,\n",
       " 'journey': 30,\n",
       " 'inspiring': 31,\n",
       " 'education': 32,\n",
       " 'institute': 33,\n",
       " 'project': 34,\n",
       " 'scientific': 35,\n",
       " '2002': 36,\n",
       " 'research': 37,\n",
       " 'space': 38,\n",
       " 'july': 39,\n",
       " 'range': 40,\n",
       " 'surface': 41,\n",
       " '2007': 42,\n",
       " 'students': 43,\n",
       " 'vision': 44,\n",
       " '2020': 45,\n",
       " 'p': 46,\n",
       " 'j': 47,\n",
       " 'jainulabdeen': 48,\n",
       " 'become': 49,\n",
       " 'is': 50,\n",
       " 'early': 51,\n",
       " 'into': 52,\n",
       " 'family': 53,\n",
       " 'during': 54,\n",
       " 'support': 55,\n",
       " 'by': 56,\n",
       " 'technology': 57,\n",
       " 'successfully': 58,\n",
       " 'air': 59,\n",
       " 'defence': 60,\n",
       " 'isro': 61,\n",
       " '1969': 62,\n",
       " '1982': 63,\n",
       " 'first': 64,\n",
       " 'satellite': 65,\n",
       " 'man': 66,\n",
       " 'including': 67,\n",
       " 'ballistic': 68,\n",
       " 'ii': 69,\n",
       " 'nuclear': 70,\n",
       " 'tests': 71,\n",
       " '1998': 72,\n",
       " 'presidency': 73,\n",
       " \"people's\": 74,\n",
       " 'numerous': 75,\n",
       " '2015': 76,\n",
       " 'public': 77,\n",
       " 'iim': 78,\n",
       " 'awards': 79,\n",
       " 'born': 80,\n",
       " 'avul': 81,\n",
       " 'pakir': 82,\n",
       " 'most': 83,\n",
       " 'life': 84,\n",
       " 'dedication': 85,\n",
       " '1931': 86,\n",
       " '1960': 87,\n",
       " 'childhood': 88,\n",
       " 'rameswaram': 89,\n",
       " 'tamil': 90,\n",
       " 'despite': 91,\n",
       " 'faced': 92,\n",
       " 'higher': 93,\n",
       " 'where': 94,\n",
       " 'especially': 95,\n",
       " 'passion': 96,\n",
       " 'madras': 97,\n",
       " 'mit': 98,\n",
       " 'but': 99,\n",
       " 'professor': 100,\n",
       " 'dream': 101,\n",
       " 'career': 102,\n",
       " '1958': 103,\n",
       " 'organisation': 104,\n",
       " 'played': 105,\n",
       " 'pivotal': 106,\n",
       " 'programs': 107,\n",
       " 'after': 108,\n",
       " 'scientist': 109,\n",
       " 'work': 110,\n",
       " 'role': 111,\n",
       " 'also': 112,\n",
       " 'be': 113,\n",
       " 'director': 114,\n",
       " 'launch': 115,\n",
       " 'vehicle': 116,\n",
       " 'slv': 117,\n",
       " 'iii': 118,\n",
       " 'under': 119,\n",
       " 'leadership': 120,\n",
       " '1999': 121,\n",
       " 'returned': 122,\n",
       " 'guided': 123,\n",
       " 'developed': 124,\n",
       " 'short': 125,\n",
       " 'these': 126,\n",
       " 'pokhran': 127,\n",
       " 'full': 128,\n",
       " 'state': 129,\n",
       " 'received': 130,\n",
       " 'serving': 131,\n",
       " '25': 132,\n",
       " 'focus': 133,\n",
       " 'youth': 134,\n",
       " 'interactions': 135,\n",
       " 'common': 136,\n",
       " 'particularly': 137,\n",
       " 'nation': 138,\n",
       " 'remained': 139,\n",
       " 'service': 140,\n",
       " 'widespread': 141,\n",
       " 'prestigious': 142,\n",
       " 'management': 143,\n",
       " 'shillong': 144,\n",
       " 'science': 145,\n",
       " 'university': 146,\n",
       " 'continued': 147,\n",
       " 'books': 148,\n",
       " 'across': 149,\n",
       " 'contribute': 150,\n",
       " 'demise': 151,\n",
       " 'honours': 152,\n",
       " 'legacy': 153,\n",
       " 'padma': 154,\n",
       " '1997': 155,\n",
       " 'award': 156,\n",
       " 'uk': 157,\n",
       " 'spirit': 158,\n",
       " 'embarked': 159,\n",
       " 'remarkable': 160,\n",
       " 'humble': 161,\n",
       " 'beginnings': 162,\n",
       " 'one': 163,\n",
       " 'revered': 164,\n",
       " 'figures': 165,\n",
       " 'story': 166,\n",
       " 'testament': 167,\n",
       " 'perseverance': 168,\n",
       " 'unwavering': 169,\n",
       " 'commitment': 170,\n",
       " \"here's\": 171,\n",
       " 'detailed': 172,\n",
       " 'look': 173,\n",
       " 'birth': 174,\n",
       " 'october': 175,\n",
       " '15': 176,\n",
       " 'nadu': 177,\n",
       " 'muslim': 178,\n",
       " 'father': 179,\n",
       " 'boat': 180,\n",
       " 'owner': 181,\n",
       " 'imam': 182,\n",
       " 'mother': 183,\n",
       " 'ashiamma': 184,\n",
       " 'housewife': 185,\n",
       " 'ancestors': 186,\n",
       " 'being': 187,\n",
       " 'wealthy': 188,\n",
       " 'traders': 189,\n",
       " 'financial': 190,\n",
       " 'hardship': 191,\n",
       " 'young': 192,\n",
       " 'sold': 193,\n",
       " 'newspapers': 194,\n",
       " 'attended': 195,\n",
       " 'schwartz': 196,\n",
       " 'secondary': 197,\n",
       " 'school': 198,\n",
       " 'ramanathapuram': 199,\n",
       " 'described': 200,\n",
       " 'bright': 201,\n",
       " 'hardworking': 202,\n",
       " 'student': 203,\n",
       " 'strong': 204,\n",
       " 'desire': 205,\n",
       " 'learn': 206,\n",
       " 'mathematics': 207,\n",
       " 'graduated': 208,\n",
       " 'physics': 209,\n",
       " 'saint': 210,\n",
       " \"joseph's\": 211,\n",
       " 'college': 212,\n",
       " 'tiruchirappalli': 213,\n",
       " '1954': 214,\n",
       " 'driven': 215,\n",
       " 'moved': 216,\n",
       " 'now': 217,\n",
       " 'chennai': 218,\n",
       " '1955': 219,\n",
       " 'study': 220,\n",
       " 'aerospace': 221,\n",
       " 'engineering': 222,\n",
       " 'senior': 223,\n",
       " 'year': 224,\n",
       " 'challenge': 225,\n",
       " 'dean': 226,\n",
       " 'completed': 227,\n",
       " 'within': 228,\n",
       " 'tight': 229,\n",
       " 'deadline': 230,\n",
       " 'impressing': 231,\n",
       " 'undone': 232,\n",
       " 'fighter': 233,\n",
       " 'pilot': 234,\n",
       " 'force': 235,\n",
       " 'narrowly': 236,\n",
       " 'missed': 237,\n",
       " 'cut': 238,\n",
       " 'placing': 239,\n",
       " 'ninth': 240,\n",
       " 'qualifiers': 241,\n",
       " 'when': 242,\n",
       " 'only': 243,\n",
       " 'eight': 244,\n",
       " 'positions': 245,\n",
       " 'were': 246,\n",
       " 'available': 247,\n",
       " 'spanned': 248,\n",
       " 'over': 249,\n",
       " 'four': 250,\n",
       " 'decades': 251,\n",
       " 'primarily': 252,\n",
       " 'roles': 253,\n",
       " 'graduating': 254,\n",
       " 'joined': 255,\n",
       " 'aeronautical': 256,\n",
       " 'establishment': 257,\n",
       " 'involved': 258,\n",
       " 'designing': 259,\n",
       " 'small': 260,\n",
       " 'hovercraft': 261,\n",
       " 'however': 262,\n",
       " 'felt': 263,\n",
       " \"wasn't\": 264,\n",
       " 'fully': 265,\n",
       " 'satisfying': 266,\n",
       " 'initiated': 267,\n",
       " 'independent': 268,\n",
       " 'expandable': 269,\n",
       " 'rocket': 270,\n",
       " '1965': 271,\n",
       " 'transferred': 272,\n",
       " 'move': 273,\n",
       " 'that': 274,\n",
       " 'proved': 275,\n",
       " 'turning': 276,\n",
       " 'point': 277,\n",
       " 'became': 278,\n",
       " 'placed': 279,\n",
       " 'rohini': 280,\n",
       " 'orbit': 281,\n",
       " '1980': 282,\n",
       " 'marking': 283,\n",
       " 'significant': 284,\n",
       " 'milestone': 285,\n",
       " 'contributed': 286,\n",
       " 'polar': 287,\n",
       " 'pslv': 288,\n",
       " 'return': 289,\n",
       " 'laboratory': 290,\n",
       " 'drdl': 291,\n",
       " 'instrumental': 292,\n",
       " 'conceiving': 293,\n",
       " 'leading': 294,\n",
       " 'integrated': 295,\n",
       " 'programme': 296,\n",
       " 'igmdp': 297,\n",
       " 'this': 298,\n",
       " 'ambitious': 299,\n",
       " 'program': 300,\n",
       " 'aimed': 301,\n",
       " 'achieving': 302,\n",
       " 'self': 303,\n",
       " 'reliance': 304,\n",
       " 'guidance': 305,\n",
       " 'series': 306,\n",
       " 'indigenous': 307,\n",
       " 'missiles': 308,\n",
       " 'prithvi': 309,\n",
       " 'agni': 310,\n",
       " 'intermediate': 311,\n",
       " 'trishul': 312,\n",
       " 'akash': 313,\n",
       " 'medium': 314,\n",
       " 'nag': 315,\n",
       " 'anti': 316,\n",
       " 'tank': 317,\n",
       " 'projects': 318,\n",
       " 'earned': 319,\n",
       " 'him': 320,\n",
       " 'title': 321,\n",
       " '1992': 322,\n",
       " 'served': 323,\n",
       " 'chief': 324,\n",
       " 'adviser': 325,\n",
       " 'prime': 326,\n",
       " 'minister': 327,\n",
       " 'secretary': 328,\n",
       " 'department': 329,\n",
       " 'organizational': 330,\n",
       " 'technical': 331,\n",
       " 'political': 332,\n",
       " 'may': 333,\n",
       " 'made': 334,\n",
       " 'fledged': 335,\n",
       " 'weapon': 336,\n",
       " 'significantly': 337,\n",
       " 'boosted': 338,\n",
       " 'recognition': 339,\n",
       " 'nominated': 340,\n",
       " 'presidential': 341,\n",
       " 'candidate': 342,\n",
       " 'ruling': 343,\n",
       " 'democratic': 344,\n",
       " 'alliance': 345,\n",
       " 'nda': 346,\n",
       " 'congress': 347,\n",
       " 'overwhelmingly': 348,\n",
       " 'elected': 349,\n",
       " '11th': 350,\n",
       " 'bachelor': 351,\n",
       " 'tenure': 352,\n",
       " 'affectionately': 353,\n",
       " 'known': 354,\n",
       " 'due': 355,\n",
       " 'accessible': 356,\n",
       " 'nature': 357,\n",
       " 'citizens': 358,\n",
       " 'actively': 359,\n",
       " 'promoted': 360,\n",
       " 'encapsulated': 361,\n",
       " 'book': 362,\n",
       " 'new': 363,\n",
       " 'millennium': 364,\n",
       " 'which': 365,\n",
       " 'outlined': 366,\n",
       " 'roadmap': 367,\n",
       " 'transforming': 368,\n",
       " 'handled': 369,\n",
       " 'controversial': 370,\n",
       " 'decision': 371,\n",
       " 'recommending': 372,\n",
       " \"president's\": 373,\n",
       " 'rule': 374,\n",
       " 'bihar': 375,\n",
       " '2005': 376,\n",
       " 'post': 377,\n",
       " 'even': 378,\n",
       " 'highly': 379,\n",
       " 'active': 380,\n",
       " 'dedicated': 381,\n",
       " 'declined': 382,\n",
       " 'opportunity': 383,\n",
       " 'second': 384,\n",
       " 'term': 385,\n",
       " 'mentorship': 386,\n",
       " 'teaching': 387,\n",
       " 'visiting': 388,\n",
       " 'various': 389,\n",
       " 'institutions': 390,\n",
       " 'ahmedabad': 391,\n",
       " 'indore': 392,\n",
       " 'iisc': 393,\n",
       " 'bangalore': 394,\n",
       " 'anna': 395,\n",
       " 'writing': 396,\n",
       " 'speaking': 397,\n",
       " 'write': 398,\n",
       " 'influential': 399,\n",
       " 'deliver': 400,\n",
       " 'motivational': 401,\n",
       " 'lectures': 402,\n",
       " 'abroad': 403,\n",
       " 'pursue': 404,\n",
       " 'their': 405,\n",
       " 'dreams': 406,\n",
       " 'building': 407,\n",
       " 'key': 408,\n",
       " 'initiatives': 409,\n",
       " 'advocating': 410,\n",
       " 'concepts': 411,\n",
       " 'like': 412,\n",
       " 'provision': 413,\n",
       " 'urban': 414,\n",
       " 'amenities': 415,\n",
       " 'rural': 416,\n",
       " 'areas': 417,\n",
       " 'pura': 418,\n",
       " 'last': 419,\n",
       " 'moments': 420,\n",
       " '27': 421,\n",
       " 'while': 422,\n",
       " 'delivering': 423,\n",
       " 'lecture': 424,\n",
       " 'creating': 425,\n",
       " 'livable': 426,\n",
       " 'planet': 427,\n",
       " 'earth': 428,\n",
       " 'collapsed': 429,\n",
       " 'massive': 430,\n",
       " 'cardiac': 431,\n",
       " 'arrest': 432,\n",
       " 'mourning': 433,\n",
       " 'sudden': 434,\n",
       " 'led': 435,\n",
       " 'grief': 436,\n",
       " 'world': 437,\n",
       " 'laid': 438,\n",
       " 'rest': 439,\n",
       " 'hometown': 440,\n",
       " 'thousands': 441,\n",
       " 'attending': 442,\n",
       " 'funeral': 443,\n",
       " 'immense': 444,\n",
       " 'multifaceted': 445,\n",
       " 'monumental': 446,\n",
       " 'contributions': 447,\n",
       " 'rocketry': 448,\n",
       " 'approachable': 449,\n",
       " 'demeanor': 450,\n",
       " 'person': 451,\n",
       " 'visionary': 452,\n",
       " 'leader': 453,\n",
       " 'continues': 454,\n",
       " 'guiding': 455,\n",
       " 'light': 456,\n",
       " 'author': 457,\n",
       " 'notably': 458,\n",
       " 'wings': 459,\n",
       " 'fire': 460,\n",
       " 'autobiography': 461,\n",
       " 'ignited': 462,\n",
       " 'minds': 463,\n",
       " 'have': 464,\n",
       " 'inspired': 465,\n",
       " 'millions': 466,\n",
       " 'bhushan': 467,\n",
       " '1981': 468,\n",
       " 'vibhushan': 469,\n",
       " '1990': 470,\n",
       " 'bharat': 471,\n",
       " 'ratna': 472,\n",
       " '–': 473,\n",
       " 'highest': 474,\n",
       " 'civilian': 475,\n",
       " 'honor': 476,\n",
       " 'indira': 477,\n",
       " 'gandhi': 478,\n",
       " 'integration': 479,\n",
       " 'veer': 480,\n",
       " 'savarkar': 481,\n",
       " 'king': 482,\n",
       " 'charles': 483,\n",
       " 'medal': 484,\n",
       " 'royal': 485,\n",
       " 'society': 486,\n",
       " 'doctor': 487,\n",
       " 'edinburgh': 488,\n",
       " '2014': 489,\n",
       " 'powerful': 490,\n",
       " 'narrative': 491,\n",
       " 'how': 492,\n",
       " 'talent': 493,\n",
       " 'hard': 494,\n",
       " 'unyielding': 495,\n",
       " 'can': 496,\n",
       " 'overcome': 497,\n",
       " 'adversity': 498,\n",
       " 'profoundly': 499,\n",
       " \"nation's\": 500,\n",
       " 'progress': 501,\n",
       " 'remains': 502,\n",
       " 'eternal': 503,\n",
       " 'source': 504,\n",
       " 'inspiration': 505,\n",
       " 'generations': 506,\n",
       " 'embodying': 507,\n",
       " 'inquiry': 508,\n",
       " 'pride': 509,\n",
       " 'selfless': 510}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2aa00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence=(tokenizer.texts_to_sequences([data])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75dbf22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25,\n",
       " 2,\n",
       " 46,\n",
       " 47,\n",
       " 22,\n",
       " 14,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 48,\n",
       " 22,\n",
       " 14,\n",
       " 159,\n",
       " 19,\n",
       " 2,\n",
       " 160,\n",
       " 30,\n",
       " 15,\n",
       " 161,\n",
       " 162,\n",
       " 4,\n",
       " 49,\n",
       " 163,\n",
       " 9,\n",
       " 20,\n",
       " 83,\n",
       " 164,\n",
       " 165,\n",
       " 6,\n",
       " 84,\n",
       " 166,\n",
       " 50,\n",
       " 2,\n",
       " 167,\n",
       " 4,\n",
       " 168,\n",
       " 85,\n",
       " 5,\n",
       " 21,\n",
       " 169,\n",
       " 170,\n",
       " 4,\n",
       " 16,\n",
       " 17,\n",
       " 171,\n",
       " 2,\n",
       " 172,\n",
       " 173,\n",
       " 23,\n",
       " 6,\n",
       " 31,\n",
       " 30,\n",
       " 51,\n",
       " 84,\n",
       " 5,\n",
       " 32,\n",
       " 86,\n",
       " 87,\n",
       " 174,\n",
       " 5,\n",
       " 88,\n",
       " 81,\n",
       " 82,\n",
       " 48,\n",
       " 22,\n",
       " 14,\n",
       " 12,\n",
       " 80,\n",
       " 19,\n",
       " 175,\n",
       " 176,\n",
       " 86,\n",
       " 8,\n",
       " 89,\n",
       " 90,\n",
       " 177,\n",
       " 52,\n",
       " 2,\n",
       " 90,\n",
       " 178,\n",
       " 53,\n",
       " 6,\n",
       " 179,\n",
       " 48,\n",
       " 12,\n",
       " 2,\n",
       " 180,\n",
       " 181,\n",
       " 5,\n",
       " 182,\n",
       " 5,\n",
       " 6,\n",
       " 183,\n",
       " 184,\n",
       " 12,\n",
       " 2,\n",
       " 185,\n",
       " 91,\n",
       " 6,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 3,\n",
       " 53,\n",
       " 92,\n",
       " 190,\n",
       " 191,\n",
       " 54,\n",
       " 6,\n",
       " 88,\n",
       " 4,\n",
       " 55,\n",
       " 6,\n",
       " 53,\n",
       " 192,\n",
       " 14,\n",
       " 193,\n",
       " 194,\n",
       " 51,\n",
       " 32,\n",
       " 7,\n",
       " 195,\n",
       " 196,\n",
       " 93,\n",
       " 197,\n",
       " 198,\n",
       " 8,\n",
       " 199,\n",
       " 94,\n",
       " 7,\n",
       " 12,\n",
       " 200,\n",
       " 18,\n",
       " 2,\n",
       " 201,\n",
       " 5,\n",
       " 202,\n",
       " 203,\n",
       " 26,\n",
       " 2,\n",
       " 204,\n",
       " 205,\n",
       " 4,\n",
       " 206,\n",
       " 95,\n",
       " 207,\n",
       " 93,\n",
       " 32,\n",
       " 7,\n",
       " 208,\n",
       " 8,\n",
       " 209,\n",
       " 15,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 8,\n",
       " 214,\n",
       " 215,\n",
       " 56,\n",
       " 6,\n",
       " 96,\n",
       " 7,\n",
       " 216,\n",
       " 4,\n",
       " 97,\n",
       " 217,\n",
       " 218,\n",
       " 8,\n",
       " 219,\n",
       " 4,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 23,\n",
       " 3,\n",
       " 97,\n",
       " 33,\n",
       " 9,\n",
       " 57,\n",
       " 98,\n",
       " 54,\n",
       " 2,\n",
       " 223,\n",
       " 224,\n",
       " 34,\n",
       " 7,\n",
       " 92,\n",
       " 2,\n",
       " 225,\n",
       " 15,\n",
       " 6,\n",
       " 226,\n",
       " 99,\n",
       " 227,\n",
       " 3,\n",
       " 34,\n",
       " 58,\n",
       " 228,\n",
       " 2,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 6,\n",
       " 100,\n",
       " 2,\n",
       " 101,\n",
       " 232,\n",
       " 27,\n",
       " 101,\n",
       " 12,\n",
       " 4,\n",
       " 49,\n",
       " 2,\n",
       " 233,\n",
       " 234,\n",
       " 8,\n",
       " 3,\n",
       " 24,\n",
       " 59,\n",
       " 235,\n",
       " 99,\n",
       " 7,\n",
       " 236,\n",
       " 237,\n",
       " 3,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 8,\n",
       " 3,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 35,\n",
       " 102,\n",
       " 103,\n",
       " 36,\n",
       " 22,\n",
       " 27,\n",
       " 35,\n",
       " 102,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 23,\n",
       " 3,\n",
       " 60,\n",
       " 37,\n",
       " 5,\n",
       " 17,\n",
       " 104,\n",
       " 28,\n",
       " 5,\n",
       " 3,\n",
       " 24,\n",
       " 38,\n",
       " 37,\n",
       " 104,\n",
       " 61,\n",
       " 94,\n",
       " 7,\n",
       " 105,\n",
       " 106,\n",
       " 253,\n",
       " 8,\n",
       " 20,\n",
       " 38,\n",
       " 5,\n",
       " 11,\n",
       " 107,\n",
       " 28,\n",
       " 103,\n",
       " 62,\n",
       " 108,\n",
       " 254,\n",
       " 15,\n",
       " 98,\n",
       " 8,\n",
       " 87,\n",
       " 7,\n",
       " 255,\n",
       " 3,\n",
       " 256,\n",
       " 17,\n",
       " 257,\n",
       " 9,\n",
       " 28,\n",
       " 18,\n",
       " 2,\n",
       " 109,\n",
       " 6,\n",
       " 51,\n",
       " 110,\n",
       " 258,\n",
       " 259,\n",
       " 2,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 7,\n",
       " 263,\n",
       " 6,\n",
       " 111,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 7,\n",
       " 112,\n",
       " 267,\n",
       " 21,\n",
       " 268,\n",
       " 34,\n",
       " 19,\n",
       " 21,\n",
       " 269,\n",
       " 270,\n",
       " 8,\n",
       " 271,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 8,\n",
       " 62,\n",
       " 14,\n",
       " 272,\n",
       " 4,\n",
       " 61,\n",
       " 2,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 4,\n",
       " 113,\n",
       " 2,\n",
       " 276,\n",
       " 277,\n",
       " 7,\n",
       " 278,\n",
       " 3,\n",
       " 34,\n",
       " 114,\n",
       " 9,\n",
       " 20,\n",
       " 64,\n",
       " 65,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 6,\n",
       " 120,\n",
       " 3,\n",
       " 117,\n",
       " 118,\n",
       " 58,\n",
       " 279,\n",
       " 3,\n",
       " 280,\n",
       " 65,\n",
       " 52,\n",
       " 281,\n",
       " 8,\n",
       " 39,\n",
       " 282,\n",
       " 283,\n",
       " 2,\n",
       " 284,\n",
       " 285,\n",
       " 8,\n",
       " 20,\n",
       " 38,\n",
       " 30,\n",
       " 7,\n",
       " 112,\n",
       " 286,\n",
       " 4,\n",
       " 3,\n",
       " 17,\n",
       " 9,\n",
       " 3,\n",
       " 287,\n",
       " 65,\n",
       " 115,\n",
       " 116,\n",
       " 288,\n",
       " 289,\n",
       " 4,\n",
       " 28,\n",
       " 5,\n",
       " 3,\n",
       " 11,\n",
       " 66,\n",
       " 9,\n",
       " 10,\n",
       " 63,\n",
       " 121,\n",
       " 14,\n",
       " 122,\n",
       " 4,\n",
       " 28,\n",
       " 8,\n",
       " 63,\n",
       " 18,\n",
       " 3,\n",
       " 114,\n",
       " 9,\n",
       " 3,\n",
       " 60,\n",
       " 37,\n",
       " 5,\n",
       " 17,\n",
       " 290,\n",
       " 291,\n",
       " 7,\n",
       " 12,\n",
       " 292,\n",
       " 8,\n",
       " 293,\n",
       " 5,\n",
       " 294,\n",
       " 3,\n",
       " 295,\n",
       " 123,\n",
       " 11,\n",
       " 17,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 23,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 8,\n",
       " 11,\n",
       " 57,\n",
       " 119,\n",
       " 6,\n",
       " 305,\n",
       " 10,\n",
       " 58,\n",
       " 124,\n",
       " 2,\n",
       " 306,\n",
       " 9,\n",
       " 307,\n",
       " 308,\n",
       " 67,\n",
       " 309,\n",
       " 2,\n",
       " 125,\n",
       " 40,\n",
       " 41,\n",
       " 4,\n",
       " 41,\n",
       " 68,\n",
       " 11,\n",
       " 310,\n",
       " 21,\n",
       " 311,\n",
       " 40,\n",
       " 68,\n",
       " 11,\n",
       " 312,\n",
       " 2,\n",
       " 125,\n",
       " 40,\n",
       " 41,\n",
       " 4,\n",
       " 59,\n",
       " 11,\n",
       " 313,\n",
       " 2,\n",
       " 314,\n",
       " 40,\n",
       " 41,\n",
       " 4,\n",
       " 59,\n",
       " 11,\n",
       " 315,\n",
       " 21,\n",
       " 316,\n",
       " 317,\n",
       " 123,\n",
       " 11,\n",
       " 6,\n",
       " 120,\n",
       " 8,\n",
       " 126,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 3,\n",
       " 321,\n",
       " 11,\n",
       " 66,\n",
       " 9,\n",
       " 10,\n",
       " 15,\n",
       " 322,\n",
       " 4,\n",
       " 121,\n",
       " 7,\n",
       " 323,\n",
       " 18,\n",
       " 3,\n",
       " 324,\n",
       " 35,\n",
       " 325,\n",
       " 4,\n",
       " 3,\n",
       " 326,\n",
       " 327,\n",
       " 5,\n",
       " 328,\n",
       " 9,\n",
       " 3,\n",
       " 329,\n",
       " 9,\n",
       " 60,\n",
       " 37,\n",
       " 17,\n",
       " 127,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 14,\n",
       " 105,\n",
       " 2,\n",
       " 106,\n",
       " 330,\n",
       " 331,\n",
       " 5,\n",
       " 332,\n",
       " 111,\n",
       " 8,\n",
       " 3,\n",
       " 127,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 8,\n",
       " 333,\n",
       " 72,\n",
       " 126,\n",
       " 71,\n",
       " 334,\n",
       " 10,\n",
       " 2,\n",
       " 128,\n",
       " 335,\n",
       " 70,\n",
       " 336,\n",
       " 129,\n",
       " 5,\n",
       " 337,\n",
       " 338,\n",
       " 27,\n",
       " 16,\n",
       " 339,\n",
       " 73,\n",
       " 36,\n",
       " 42,\n",
       " 3,\n",
       " 74,\n",
       " 29,\n",
       " 8,\n",
       " 36,\n",
       " 2,\n",
       " 46,\n",
       " 47,\n",
       " 22,\n",
       " 14,\n",
       " 12,\n",
       " 340,\n",
       " 18,\n",
       " 3,\n",
       " 341,\n",
       " 342,\n",
       " 56,\n",
       " 3,\n",
       " 343,\n",
       " 16,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 5,\n",
       " 130,\n",
       " 55,\n",
       " 15,\n",
       " 3,\n",
       " 24,\n",
       " 16,\n",
       " 347,\n",
       " 7,\n",
       " 12,\n",
       " 348,\n",
       " 349,\n",
       " 18,\n",
       " 3,\n",
       " 350,\n",
       " 29,\n",
       " 9,\n",
       " 10,\n",
       " 131,\n",
       " 15,\n",
       " 39,\n",
       " 132,\n",
       " 36,\n",
       " 4,\n",
       " 39,\n",
       " 132,\n",
       " 42,\n",
       " 7,\n",
       " 12,\n",
       " 3,\n",
       " 64,\n",
       " 109,\n",
       " 5,\n",
       " 3,\n",
       " 64,\n",
       " 351,\n",
       " 4,\n",
       " 49,\n",
       " 20,\n",
       " 29,\n",
       " 54,\n",
       " 6,\n",
       " 352,\n",
       " 7,\n",
       " 12,\n",
       " 353,\n",
       " 354,\n",
       " 18,\n",
       " 3,\n",
       " 74,\n",
       " 29,\n",
       " 355,\n",
       " 4,\n",
       " 6,\n",
       " 356,\n",
       " 357,\n",
       " 6,\n",
       " 133,\n",
       " 19,\n",
       " 31,\n",
       " 3,\n",
       " 134,\n",
       " 5,\n",
       " 6,\n",
       " 75,\n",
       " 135,\n",
       " 26,\n",
       " 136,\n",
       " 358,\n",
       " 137,\n",
       " 43,\n",
       " 7,\n",
       " 359,\n",
       " 360,\n",
       " 6,\n",
       " 44,\n",
       " 13,\n",
       " 10,\n",
       " 361,\n",
       " 8,\n",
       " 6,\n",
       " 362,\n",
       " 10,\n",
       " 45,\n",
       " 2,\n",
       " 44,\n",
       " 13,\n",
       " 3,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 2,\n",
       " 367,\n",
       " 13,\n",
       " 368,\n",
       " 10,\n",
       " 52,\n",
       " 2,\n",
       " 124,\n",
       " 138,\n",
       " 7,\n",
       " 369,\n",
       " 2,\n",
       " 370,\n",
       " 371,\n",
       " 56,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 8,\n",
       " 375,\n",
       " 8,\n",
       " 376,\n",
       " 377,\n",
       " 73,\n",
       " 42,\n",
       " 76,\n",
       " 378,\n",
       " 108,\n",
       " 6,\n",
       " 73,\n",
       " 25,\n",
       " 14,\n",
       " 139,\n",
       " 379,\n",
       " 380,\n",
       " 5,\n",
       " 381,\n",
       " 4,\n",
       " 77,\n",
       " 140,\n",
       " 7,\n",
       " 382,\n",
       " 3,\n",
       " 383,\n",
       " 13,\n",
       " 2,\n",
       " 384,\n",
       " 385,\n",
       " 91,\n",
       " 141,\n",
       " 77,\n",
       " 55,\n",
       " 32,\n",
       " 5,\n",
       " 386,\n",
       " 7,\n",
       " 122,\n",
       " 4,\n",
       " 6,\n",
       " 96,\n",
       " 13,\n",
       " 387,\n",
       " 131,\n",
       " 18,\n",
       " 2,\n",
       " 388,\n",
       " 100,\n",
       " 23,\n",
       " 389,\n",
       " 142,\n",
       " 390,\n",
       " 67,\n",
       " 24,\n",
       " 33,\n",
       " 9,\n",
       " 143,\n",
       " 78,\n",
       " 391,\n",
       " 78,\n",
       " 144,\n",
       " 78,\n",
       " 392,\n",
       " 24,\n",
       " 33,\n",
       " 9,\n",
       " 145,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 146,\n",
       " 396,\n",
       " 5,\n",
       " 77,\n",
       " 397,\n",
       " 7,\n",
       " 147,\n",
       " 4,\n",
       " 398,\n",
       " 399,\n",
       " 148,\n",
       " 5,\n",
       " 400,\n",
       " 75,\n",
       " 401,\n",
       " 402,\n",
       " 149,\n",
       " 10,\n",
       " 5,\n",
       " 403,\n",
       " 137,\n",
       " 4,\n",
       " 43,\n",
       " 6,\n",
       " 133,\n",
       " 139,\n",
       " 19,\n",
       " 31,\n",
       " 134,\n",
       " 4,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 138,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 7,\n",
       " 147,\n",
       " 410,\n",
       " 13,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 9,\n",
       " 414,\n",
       " 415,\n",
       " 4,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 5,\n",
       " 57,\n",
       " 44,\n",
       " 45,\n",
       " 151,\n",
       " 76,\n",
       " 419,\n",
       " 420,\n",
       " 19,\n",
       " 39,\n",
       " 421,\n",
       " 76,\n",
       " 422,\n",
       " 423,\n",
       " 2,\n",
       " 424,\n",
       " 19,\n",
       " 425,\n",
       " 2,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 4,\n",
       " 43,\n",
       " 23,\n",
       " 3,\n",
       " 24,\n",
       " 33,\n",
       " 9,\n",
       " 143,\n",
       " 144,\n",
       " 25,\n",
       " 14,\n",
       " 429,\n",
       " 15,\n",
       " 2,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 16,\n",
       " 433,\n",
       " 6,\n",
       " 434,\n",
       " 151,\n",
       " 435,\n",
       " 4,\n",
       " 141,\n",
       " 436,\n",
       " 149,\n",
       " 10,\n",
       " 5,\n",
       " 3,\n",
       " 437,\n",
       " 7,\n",
       " 12,\n",
       " 438,\n",
       " 4,\n",
       " 439,\n",
       " 26,\n",
       " 128,\n",
       " 129,\n",
       " 152,\n",
       " 8,\n",
       " 6,\n",
       " 440,\n",
       " 9,\n",
       " 89,\n",
       " 26,\n",
       " 441,\n",
       " 442,\n",
       " 6,\n",
       " 443,\n",
       " 153,\n",
       " 5,\n",
       " 79,\n",
       " 25,\n",
       " 27,\n",
       " 153,\n",
       " 50,\n",
       " 444,\n",
       " 5,\n",
       " 445,\n",
       " 11,\n",
       " 66,\n",
       " 9,\n",
       " 10,\n",
       " 13,\n",
       " 6,\n",
       " 446,\n",
       " 447,\n",
       " 4,\n",
       " 20,\n",
       " 68,\n",
       " 11,\n",
       " 5,\n",
       " 38,\n",
       " 448,\n",
       " 107,\n",
       " 74,\n",
       " 29,\n",
       " 13,\n",
       " 6,\n",
       " 449,\n",
       " 450,\n",
       " 85,\n",
       " 4,\n",
       " 3,\n",
       " 136,\n",
       " 451,\n",
       " 5,\n",
       " 31,\n",
       " 135,\n",
       " 95,\n",
       " 26,\n",
       " 43,\n",
       " 452,\n",
       " 453,\n",
       " 6,\n",
       " 10,\n",
       " 45,\n",
       " 44,\n",
       " 454,\n",
       " 4,\n",
       " 113,\n",
       " 2,\n",
       " 455,\n",
       " 456,\n",
       " 13,\n",
       " 16,\n",
       " 17,\n",
       " 457,\n",
       " 6,\n",
       " 148,\n",
       " 83,\n",
       " 458,\n",
       " 459,\n",
       " 9,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 5,\n",
       " 10,\n",
       " 45,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 79,\n",
       " 5,\n",
       " 152,\n",
       " 7,\n",
       " 130,\n",
       " 75,\n",
       " 142,\n",
       " 79,\n",
       " 67,\n",
       " 154,\n",
       " 467,\n",
       " 468,\n",
       " 154,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 155,\n",
       " 473,\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2850a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1080"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c4d09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save the tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a55a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(sequence, window_size=100):\n",
    "    input,labels=[],[]\n",
    "    for i in range(len(sequence)-window_size):\n",
    "        input.append(sequence[i:i+window_size])\n",
    "        labels.append(sequence[i+1:i+window_size+1])\n",
    "    return np.array(input), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0642f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data,y_data=create_dataset(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9448a5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "263dd776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04e7a793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 25,   2,  46, ...,  91,   6, 186],\n",
       "       [  2,  46,  47, ...,   6, 186, 187],\n",
       "       [ 46,  47,  22, ..., 186, 187, 188],\n",
       "       ...,\n",
       "       [ 45, 464, 465, ..., 508,  16, 509],\n",
       "       [464, 465, 466, ...,  16, 509,   5],\n",
       "       [465, 466,  79, ..., 509,   5, 510]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0760967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,  46,  47, ...,   6, 186, 187],\n",
       "       [ 46,  47,  22, ..., 186, 187, 188],\n",
       "       [ 47,  22,  14, ..., 187, 188, 189],\n",
       "       ...,\n",
       "       [464, 465, 466, ...,  16, 509,   5],\n",
       "       [465, 466,  79, ..., 509,   5, 510],\n",
       "       [466,  79,   5, ...,   5, 510, 140]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a521b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 25,   2,  46,  47,  22,  14,  80,  81,  82,  48,  22,  14, 159,\n",
       "        19,   2, 160,  30,  15, 161, 162,   4,  49, 163,   9,  20,  83,\n",
       "       164, 165,   6,  84, 166,  50,   2, 167,   4, 168,  85,   5,  21,\n",
       "       169, 170,   4,  16,  17, 171,   2, 172, 173,  23,   6,  31,  30,\n",
       "        51,  84,   5,  32,  86,  87, 174,   5,  88,  81,  82,  48,  22,\n",
       "        14,  12,  80,  19, 175, 176,  86,   8,  89,  90, 177,  52,   2,\n",
       "        90, 178,  53,   6, 179,  48,  12,   2, 180, 181,   5, 182,   5,\n",
       "         6, 183, 184,  12,   2, 185,  91,   6, 186])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d931f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,  46,  47,  22,  14,  80,  81,  82,  48,  22,  14, 159,  19,\n",
       "         2, 160,  30,  15, 161, 162,   4,  49, 163,   9,  20,  83, 164,\n",
       "       165,   6,  84, 166,  50,   2, 167,   4, 168,  85,   5,  21, 169,\n",
       "       170,   4,  16,  17, 171,   2, 172, 173,  23,   6,  31,  30,  51,\n",
       "        84,   5,  32,  86,  87, 174,   5,  88,  81,  82,  48,  22,  14,\n",
       "        12,  80,  19, 175, 176,  86,   8,  89,  90, 177,  52,   2,  90,\n",
       "       178,  53,   6, 179,  48,  12,   2, 180, 181,   5, 182,   5,   6,\n",
       "       183, 184,  12,   2, 185,  91,   6, 186, 187])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19a2c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        pos=np.arange(max_len)[:, np.newaxis]\n",
    "        i=np.arange(d_model)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model)) \n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "    def call(self,x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caaa5581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def transformer_block(embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "    inputs = layers.Input(shape=(None, embed_dim))\n",
    "    \n",
    "    # Multi-head self-attention\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "    attn_output = layers.Dropout(dropout)(attn_output)\n",
    "    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ffn = tf.keras.Sequential([\n",
    "        layers.Dense(ff_dim, activation='relu'),\n",
    "        layers.Dense(embed_dim)\n",
    "    ])\n",
    "    ffn_output = ffn(out1)\n",
    "    ffn_output = layers.Dropout(dropout)(ffn_output)\n",
    "    \n",
    "    # Output after second residual connection + normalization\n",
    "    out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=out2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "094f498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10000\n",
    "max_seq_length=100\n",
    "num_heads=16\n",
    "ff_dim=512\n",
    "num_layers=4\n",
    "batch_size=64\n",
    "epochs=10\n",
    "embed_dim=100\n",
    "\n",
    "def build_gpt_model():\n",
    "    input=layers.Input(shape=(max_seq_length, ))\n",
    "    x=layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(input)\n",
    "    \n",
    "    x=PositionalEncoding(max_seq_length,embed_dim)(x)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x=transformer_block(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)(x)\n",
    "    output=layers.Dense(vocab_size, activation='softmax')(x)\n",
    "    return tf.keras.Model(inputs=input, outputs=output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b4eae24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\ananconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=build_gpt_model()\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7240a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 5s/step - accuracy: 0.0076 - loss: 8.3369 - val_accuracy: 0.0129 - val_loss: 8.0107\n",
      "Epoch 2/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 4s/step - accuracy: 0.0262 - loss: 6.6237 - val_accuracy: 0.0129 - val_loss: 7.6689\n",
      "Epoch 3/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3s/step - accuracy: 0.0391 - loss: 5.8092 - val_accuracy: 0.0145 - val_loss: 7.6884\n",
      "Epoch 4/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3s/step - accuracy: 0.0425 - loss: 5.5247 - val_accuracy: 0.0145 - val_loss: 7.7811\n",
      "Epoch 5/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 3s/step - accuracy: 0.0434 - loss: 5.4686 - val_accuracy: 0.0145 - val_loss: 7.8964\n",
      "Epoch 6/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 4s/step - accuracy: 0.0437 - loss: 5.4527 - val_accuracy: 0.0145 - val_loss: 8.0278\n",
      "Epoch 7/10\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_data, y_data, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gpt_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322f096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_encoding_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">748,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">748,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">748,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">748,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_11 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m1,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_encoding_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_1 (\u001b[38;5;33mFunctional\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │       \u001b[38;5;34m748,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_3 (\u001b[38;5;33mFunctional\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │       \u001b[38;5;34m748,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_5 (\u001b[38;5;33mFunctional\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │       \u001b[38;5;34m748,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_7 (\u001b[38;5;33mFunctional\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │       \u001b[38;5;34m748,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m10000\u001b[0m)     │     \u001b[38;5;34m1,010,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,009,746</span> (57.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,009,746\u001b[0m (57.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,003,248</span> (19.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,003,248\u001b[0m (19.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,006,498</span> (38.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m10,006,498\u001b[0m (38.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd873393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, model, tokenizer, num_tokens=50, temperature=1.0):\n",
    "    for _ in range(num_tokens):\n",
    "        token_seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_seq = token_seq[-max_seq_length:]  # Trim to max length\n",
    "        padded_seq = pad_sequences([token_seq], maxlen=max_seq_length)\n",
    "\n",
    "        preds = model.predict(padded_seq, verbose=0)[0, -1]  # Get prediction for last time step\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "\n",
    "        # Apply temperature sampling\n",
    "        preds = np.log(preds + 1e-9) / temperature\n",
    "        preds = np.exp(preds) / np.sum(np.exp(preds))\n",
    "\n",
    "        next_token_id = np.random.choice(len(preds), p=preds)\n",
    "        next_word = tokenizer.index_word.get(next_token_id, '')\n",
    "\n",
    "        seed_text += ' ' + next_word\n",
    "\n",
    "        if next_word == '':  # Optional: break if OOV or unknown word\n",
    "            break\n",
    "\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75cd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Generated Text:\n",
      "who is APJ Abdul Kalam program of missile played of opportunity to in serving prime kalam nominated undone the space rule elected to active 1992 interactions a planet air to is missile 2020 only presidency early in controversial drdo to despite 25 elected detailed nadu iim in congress support rohini graduating scientific involved born his\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"who is APJ Abdul Kalam\"\n",
    "generated = generate_text(seed_text, model, tokenizer, num_tokens=50, temperature=1.0)\n",
    "\n",
    "print(\"📝 Generated Text:\")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8476ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing the pad_sequences\n",
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31adc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
